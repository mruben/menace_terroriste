{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## CHARGEMENT DES BIBLIOTHEQUES\n",
    "############################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from umap import UMAP\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FONCTIONS UTILES\n",
    "############################################################\n",
    "\n",
    "##\n",
    "# CHARGEMENT DE FICHIERS JSON\n",
    "\n",
    "# Fonction qui permet de charger un fichier json dans un dictionnaire\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "##\n",
    "# CHARGEMENT DU DATAFRAME\n",
    "\n",
    "# On charge les données dans un dataframe \"df\"\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "##\n",
    "# FILTRAGE DE COLONNES\n",
    "\n",
    "# On récupère ici seulement les évènements qui nous intéressent, ceux liés à des violences politiques\n",
    "# On conserve 'Strategic developments' car elle permet de capturer le contexte\n",
    "def columns_filtering(df):\n",
    "    events_filter = ['Battles', 'Explosions/Remote violence',\n",
    "                    'Violence against civilians', 'Strategic developments']\n",
    "\n",
    "    df = df[df['event_type'].isin(events_filter)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "##\n",
    "# CREATION DE NOUVELLES COLONNES\n",
    "\n",
    "def add_new_colums_dates(df):\n",
    "    ##\n",
    "    # On transforme la colonne \"event_date\" en datetime pour la manipuler plus facilement\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "\n",
    "    # On crée la colonne \"month\" en utilisant l'attribut month de datetime\n",
    "    df['month'] = df['event_date'].dt.month\n",
    "    # On ajoute la colonne juste après \"year\"\n",
    "    df.insert(3, 'month', df.pop('month'))\n",
    "\n",
    "    # On crée la colonne \"day\" en utilisant l'attribut day de datetime\n",
    "    df['day'] = df['event_date'].dt.day\n",
    "    # On ajoute la colonne juste après \"month\"\n",
    "    df.insert(5, 'day', df.pop('day'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_colums_actors(df):\n",
    "    ##\n",
    "    # On charge les données du fichier json \"actor_type\" dans un dictionnaire\n",
    "    # qui associe les valeurs des colonnes \"inter1\" et \"inter2\"\n",
    "    # au nom de chaque catégorie d'acteur (source : codebook ACLED)\n",
    "    actor_type = load_json_file(\"../references/actor_type.json\")\n",
    "\n",
    "    # On convertit les clés du dictionnaire en entiers\n",
    "    actor_type = {int(k): v for k, v in actor_type.items()}\n",
    "\n",
    "    # On ajoute les colonnes \"actor1_type\" et \"actor2_type\" au dataframe\n",
    "    df['actor1_type'] = df['inter1'].map(actor_type)\n",
    "    df['actor2_type'] = df['inter2'].map(actor_type)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_colums_interaction(df):\n",
    "    ##\n",
    "    # On charge les données du fichier json \"interaction_type\" dans un dictionnaire\n",
    "    # associant aux valeurs de la colonne \"interaction\"\n",
    "    # les 2 acteurs impliqués dans une confrontation (source : codebook ACLED)\n",
    "    interaction_type = load_json_file(\"../references/interaction_type.json\")\n",
    "\n",
    "    # On convertit les clés du dictionnaire en entiers\n",
    "    interaction_type = {int(k): v for k, v in interaction_type.items()}\n",
    "\n",
    "    # On ajoute une colonne \"interaction_type\" au dataframe\n",
    "    df['interaction_type'] = df['interaction'].map(interaction_type)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_colums_terrorist_group(df):\n",
    "    # On crée une nouvelle colonne qui indique si pour un évènement l'un des\n",
    "    # acteurs est une organisation terroriste\n",
    "    # On charge les données du fichier json \"terrorist_group_filiation\"\n",
    "    # dans un dictionnaire associant organisation terroriste et organisation mère\n",
    "    terrorist_group_filiation = load_json_file(\"../references/terrorist_group_filiation.json\")\n",
    "\n",
    "    # On crée une liste contenant les organisations terroristes à partir des indices de ce dictionnaire\n",
    "    terrorist_groups = list(terrorist_group_filiation.keys())\n",
    "\n",
    "    # On crée une fonction pour vérifier si un acteur est une organisation terroriste\n",
    "    def is_terrorist_actor(actor):\n",
    "        return actor in terrorist_groups\n",
    "\n",
    "    # On ajoute la colonne \"is_terrorist_group_related\" au dataframe\n",
    "    df['is_terrorist_group_related'] = (df['actor1'].apply(is_terrorist_actor) |\n",
    "                                df['assoc_actor_1'].apply(is_terrorist_actor) |\n",
    "                                df['actor2'].apply(is_terrorist_actor) |\n",
    "                                df['assoc_actor_2'].apply(is_terrorist_actor)).astype(int)\n",
    "\n",
    "    # On crée une fonction de mapping pour associer les valeurs du dictionnaire aux acteurs\n",
    "    # S'il n'y a pas de valeur on retourne \"None\" car cela veut simplement dire que l'évènement\n",
    "    # n'est pas lié à une organisation terroriste et qu'il n'y a donc pas de lien de filiation\n",
    "    # avec une organisation mère\n",
    "    def map_filiation(row):\n",
    "        for actor in ['actor1', 'assoc_actor_1', 'actor2', 'assoc_actor_2']:\n",
    "            if row[actor] in terrorist_group_filiation:\n",
    "                return terrorist_group_filiation[row[actor]]\n",
    "        return \"None\"\n",
    "\n",
    "    # On ajoute la colonne \"terrorist_group_filiation\" au dataframe\n",
    "    df['terrorist_group_filiation'] = df.apply(map_filiation, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_colums_pmc_group(df):\n",
    "    # On crée une nouvelle colonne qui indique si pour un évènement l'un\n",
    "    # des acteurs est une pmc russe\n",
    "    pmc_groups = ['Wagner Group']\n",
    "\n",
    "    # On crée une fonction pour vérifier si un acteur est une pmc russe\n",
    "    def is_pmc_actor(actor):\n",
    "        return actor in pmc_groups\n",
    "\n",
    "    # On ajoute la colonne \"is_pmc_related\" au dataframe\n",
    "    df['is_pmc_related'] = (df['actor1'].apply(is_pmc_actor) |\n",
    "                                df['assoc_actor_1'].apply(is_pmc_actor) |\n",
    "                                df['actor2'].apply(is_pmc_actor) |\n",
    "                                df['assoc_actor_2'].apply(is_pmc_actor)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "##\n",
    "# SUPPRESSION DE COLONNES\n",
    "\n",
    "def delete_columns(df):\n",
    "    # On supprime les colonnes qui ne nous serons d'aucune utilité\n",
    "    columns_to_drop = [\n",
    "        'time_precision',\n",
    "        'disorder_type',\n",
    "        'sub_event_type',\n",
    "        'actor1',\n",
    "        'assoc_actor_1',\n",
    "        'actor2',\n",
    "        'assoc_actor_2',\n",
    "        'inter1',\n",
    "        'inter2',\n",
    "        'interaction',\n",
    "        'admin2',\n",
    "        'admin3',\n",
    "        'iso',\n",
    "        'region',\n",
    "        'location',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'geo_precision',\n",
    "        'source',\n",
    "        'source_scale',\n",
    "        'notes',\n",
    "        'tags',\n",
    "        'timestamp',\n",
    "        'civilian_targeting',\n",
    "        'event_id_cnty'\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "##\n",
    "# GESTION DES VALEURS MANQUANTES\n",
    "\n",
    "def fill_nan_values(df):\n",
    "    # Pour la colonne \"actor2_type\" on va remplacer les valeurs manquantes par \"None\" car cela\n",
    "    # indique juste que dans certains cas qu'il y a un seul acteur et pas un manque de valeur\n",
    "    df['actor2_type'] = df['actor2_type'].fillna(\"None\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "##\n",
    "# Fonction pour mapper les noms de sous-régions aux identifiants de coordonnées\n",
    "def map_regions_to_ids(data, mapping):\n",
    "    data['region_id'] = data['admin1'].map(mapping)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTRUCTION DU DATAFRAME HISTORIQUE \"past_data\"\n",
    "############################################################\n",
    "\n",
    "def build_past_data(df):\n",
    "    # On compte les occurences de chaque valeur des colonnes 'actor1_type' et 'actor2_type'\n",
    "    actor_counts = df.melt(id_vars=['event_date', 'country', 'admin1', 'year', 'month', 'is_terrorist_group_related', 'fatalities', 'event_type'], \n",
    "                        value_vars=['actor1_type', 'actor2_type'], \n",
    "                        var_name='actor_role', value_name='actor')\n",
    "\n",
    "    actor_counts = actor_counts.groupby(['country', 'admin1', 'year', 'month', 'actor']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # On compte les occurences de chaque type d'évènement\n",
    "    event_type_counts = df.pivot_table(index=['country', 'admin1', 'year', 'month'], \n",
    "                                    columns='event_type', \n",
    "                                    aggfunc='size', \n",
    "                                    fill_value=0).reset_index()\n",
    "\n",
    "    # On agrège les données par mois, par pays, par sous-région\n",
    "    past_data = df.groupby(['country', 'admin1', 'year', 'month']).agg(\n",
    "        total_events=('event_date', 'count'),\n",
    "        terrorist_events=('is_terrorist_group_related', 'sum'),\n",
    "        fatalities=('fatalities', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # On fusionne les variables des acteurs et des types d'évènements avec les données agrégées\n",
    "    past_data = past_data.merge(actor_counts, on=['country', 'admin1', 'year', 'month'], how='left')\n",
    "    past_data = past_data.merge(event_type_counts, on=['country', 'admin1', 'year', 'month'], how='left')\n",
    "\n",
    "    # On supprime les colonnes 'None' et 'Protesters' qui concernent les acteurs mais sont sans intérêt pour notre étude\n",
    "    # ou bien la colonne 'Rebel Groups' qui est dérivée de la variable cible\n",
    "    past_data = past_data.drop(columns=['None', 'Protesters', 'Rebel Groups'], axis=1)\n",
    "\n",
    "    # On crée des features décalées (lags) pour capturer les tendances passées des événements totaux et des événements terroristes\n",
    "    for admin in past_data['admin1'].unique():  # Pour chaque valeur unique dans la colonne 'admin1'\n",
    "        \n",
    "        for lag in range(1, 7):  # Pour chaque décalage (lag) de 1 à 6\n",
    "            lag_total_events = past_data[past_data['admin1'] == admin]['total_events'].shift(lag)  # Crée une série décalée pour 'total_events' pour l'admin actuel\n",
    "            lag_terrorist_events = past_data[past_data['admin1'] == admin]['terrorist_events'].shift(lag)  # Crée une série décalée pour 'terrorist_events' sans filtrer par admin\n",
    "            past_data.loc[past_data['admin1'] == admin, f'total_events_lag_{lag}'] = lag_total_events  # Ajoute la série décalée pour 'total_events' dans une nouvelle colonne pour l'admin actuel\n",
    "            past_data.loc[past_data['admin1'] == admin, f'terrorist_events_lag_{lag}'] = lag_terrorist_events  # Ajoute la série décalée pour 'terrorist_events' dans une nouvelle colonne pour l'admin actuel\n",
    "            past_data[[f'total_events_lag_{lag}', f'terrorist_events_lag_{lag}']] = past_data[\n",
    "                [f'total_events_lag_{lag}', f'terrorist_events_lag_{lag}']\n",
    "                ].fillna(0)  # Remplit les valeurs manquantes dans les colonnes décalées avec des zéros\n",
    "\n",
    "    # On supprime les valeurs NA générées par les lags\n",
    "    # past_data.dropna(inplace=True)  # Optionnel : supprime les lignes contenant des valeurs NA (commenté ici)\n",
    "\n",
    "    # On supprime la colonne \"total_events\" qui est trop corrélée à notre variable cible\n",
    "    past_data = past_data.drop(columns=['total_events'], axis=1)\n",
    "\n",
    "    return past_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSFORMATION DES DONNEES DU DATAFRAME \"past_data\"\n",
    "############################################################\n",
    "\n",
    "def past_data_transformation(df):\n",
    "\n",
    "    ##\n",
    "    # On trie notre dataframe par année puis mois dans l'ordre croissant\n",
    "    # pour que les données de test concernent les événements les plus récents\n",
    "    df = df.sort_values(by=['year', 'month'])\n",
    "\n",
    "    # Séparation des variables explicatives de notre variable cible\n",
    "    X = df.drop(columns='terrorist_events', axis=1)\n",
    "    y = df['terrorist_events']\n",
    "\n",
    "    # Division en ensembles d'entraînement et de test avec shuffle à False\n",
    "    # pour que les données de test concernent les événements les plus récents (6 derniers mois)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # On sauvegarde une version non encodée de X_test\n",
    "    X_test_no_encoding = X_test\n",
    "\n",
    "\n",
    "    ##\n",
    "    # On standardise les variables numériques exceptées les variables catégorielles\n",
    "    # ainsi que l'année et le mois qui subiront une autre transformation\n",
    "    cat_columns=['country', 'admin1']\n",
    "    X_train_numerical_vars = [col for col in X_train.columns if col not in cat_columns + ['year', 'month']]\n",
    "    X_test_numerical_vars = [col for col in X_test.columns if col not in cat_columns + ['year', 'month']]\n",
    "    # Extraction des données numériques\n",
    "    X_train_numerical = X_train[X_train_numerical_vars].values\n",
    "    X_test_numerical = X_test[X_test_numerical_vars].values\n",
    "\n",
    "    # Initialisation d'un scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train[X_train_numerical_vars] = scaler.fit_transform( X_train_numerical)\n",
    "    X_test[X_test_numerical_vars] = scaler.transform(X_test_numerical)\n",
    "\n",
    "\n",
    "    ##\n",
    "    # Hot One Encoding des variables catégorielles\n",
    "    X_train_encoded = pd.get_dummies(X_train, columns=cat_columns, dtype=int)\n",
    "    X_test_encoded = pd.get_dummies(X_test, columns=cat_columns, dtype=int)\n",
    "    # On aligne les colonnes de X_test_encoded sur celles de X_train_encoded\n",
    "    # pour s'assurer qu'ils ont les mêmes colonnes\n",
    "    X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "\n",
    "    ##\n",
    "    # Transformation trigonométrique de 'year' et 'month' pour la saisonnalité\n",
    "    X_train_encoded['month_sin'] = np.sin(2 * np.pi * X_train_encoded['month'] / 12)\n",
    "    X_train_encoded['month_cos'] = np.cos(2 * np.pi * X_train_encoded['month'] / 12)\n",
    "    X_train_encoded['year_sin'] = np.sin(2 * np.pi * (X_train_encoded['year'] - X_train_encoded['year'].min()) / \n",
    "                                    (X_train_encoded['year'].max() - X_train_encoded['year'].min() + 1))\n",
    "    X_train_encoded['year_cos'] = np.cos(2 * np.pi * (X_train_encoded['year'] - X_train_encoded['year'].min()) / \n",
    "                                    (X_train_encoded['year'].max() - X_train_encoded['year'].min() + 1))\n",
    "    \n",
    "    X_test_encoded['month_sin'] = np.sin(2 * np.pi * X_test_encoded['month'] / 12)\n",
    "    X_test_encoded['month_cos'] = np.cos(2 * np.pi * X_test_encoded['month'] / 12)\n",
    "    X_test_encoded['year_sin'] = np.sin(2 * np.pi * (X_test_encoded['year'] - X_test_encoded['year'].min()) / \n",
    "                                    (X_test_encoded['year'].max() - X_test_encoded['year'].min() + 1))\n",
    "    X_test_encoded['year_cos'] = np.cos(2 * np.pi * (X_test_encoded['year'] - X_test_encoded['year'].min()) / \n",
    "                                    (X_test_encoded['year'].max() - X_test_encoded['year'].min() + 1))\n",
    "\n",
    "    # Suppression des colonnes 'month' et 'year' après transformation\n",
    "    X_train_encoded = X_train_encoded.drop(columns=['month', 'year'])\n",
    "    X_test_encoded = X_test_encoded.drop(columns=['month', 'year'])\n",
    "\n",
    "    return X_train_encoded, X_test_encoded, X_test_no_encoding, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTRUCTION DU DATAFRAME FUTUR \"future_data\"\n",
    "## POUR LES PREDICTIONS DES 6 PROCHAINS MOIS\n",
    "############################################################\n",
    "\n",
    "def build_future_data(past_data):\n",
    "    unique_country_admin1 = past_data[['country', 'admin1']].drop_duplicates()\n",
    "\n",
    "    # On souhaite d'abord obtenir le dernier mois de la dernière l'année dans past_data\n",
    "    last_year = past_data['year'].max()\n",
    "    last_month = past_data[past_data['year'] == last_year]['month'].max()\n",
    "\n",
    "    # On crée ensuite une liste de dictionnaires pour les 6 prochains mois avec les colonnes explicatives vides\n",
    "    future_data_list = []\n",
    "\n",
    "    # On ajoute les 6 prochains mois à future_data_list avec les colonnes connues \"country\" et \"admin1\" remplies\n",
    "    for i in range(1, 7):\n",
    "        next_month = last_month + i\n",
    "        next_year = last_year\n",
    "        if next_month > 12:\n",
    "            next_month -= 12\n",
    "            next_year += 1\n",
    "        for _, row in unique_country_admin1.iterrows():\n",
    "            future_data_list.append({\n",
    "                'year': next_year, \n",
    "                'month': next_month, \n",
    "                'country': row['country'], \n",
    "                'admin1': row['admin1']\n",
    "            })\n",
    "\n",
    "    # On crée notre dataframe \"future_data\" à partir de la liste de dictionnaires\n",
    "    future_data = pd.DataFrame(future_data_list, columns=past_data.columns)\n",
    "\n",
    "    # On remplit les autres colonnes explicatives avec des valeurs manquantes\n",
    "    for col in past_data.columns:\n",
    "        if col not in ['year', 'month', 'country', 'admin1']:\n",
    "            future_data[col] = np.nan\n",
    "\n",
    "    # Liste des colonnes pour lesquelles on veut remplir les valeurs manquantes\n",
    "    # columns_to_fill = [\n",
    "    #     'total_events', 'fatalities', 'Civilians', 'External/Other Forces',\n",
    "    #     'Identity Militias', 'Political Militias',\n",
    "    #     'State Forces', 'Battles', 'Explosions/Remote violence',\n",
    "    #     'Strategic developments', 'Violence against civilians',\n",
    "    #     'total_events_lag_1', 'terrorist_events_lag_1',\n",
    "    #     'total_events_lag_2', 'terrorist_events_lag_2',\n",
    "    #     'total_events_lag_3', 'terrorist_events_lag_3',\n",
    "    #     'total_events_lag_4', 'terrorist_events_lag_4',\n",
    "    #     'total_events_lag_5', 'terrorist_events_lag_5',\n",
    "    #     'total_events_lag_6', 'terrorist_events_lag_6'\n",
    "    # ]\n",
    "\n",
    "    columns_to_fill = [\n",
    "        'fatalities', 'Civilians', 'External/Other Forces',\n",
    "        'Identity Militias', 'Political Militias',\n",
    "        'State Forces', 'Battles', 'Explosions/Remote violence',\n",
    "        'Strategic developments', 'Violence against civilians',\n",
    "        'total_events_lag_1', 'terrorist_events_lag_1',\n",
    "        'total_events_lag_2', 'terrorist_events_lag_2',\n",
    "        'total_events_lag_3', 'terrorist_events_lag_3',\n",
    "        'total_events_lag_4', 'terrorist_events_lag_4',\n",
    "        'total_events_lag_5', 'terrorist_events_lag_5',\n",
    "        'total_events_lag_6', 'terrorist_events_lag_6'\n",
    "    ]\n",
    "\n",
    "\n",
    "    # On groupe les données par 'country', 'admin1', 'month' pour calculer les moyennes dans past_data\n",
    "    grouped_data = past_data.groupby(['country', 'admin1', 'month']).mean().reset_index()\n",
    "\n",
    "    # On remplace les valeurs manquantes dans future_data par les moyennes correspondantes de past_data\n",
    "    # en fonction du mois. Ex : si on veut calculer les valeurs des variables du mois de juillet 2024,\n",
    "    # on va calculer la moyenne des variables des mois de juillet des années précédentes pour avoir des\n",
    "    # valeurs de test réalistes.\n",
    "    for index, row in future_data.iterrows():\n",
    "        country = row['country']\n",
    "        admin1 = row['admin1']\n",
    "        month = row['month']\n",
    "        for var in columns_to_fill:\n",
    "            mean_value = grouped_data[(grouped_data['country'] == country) & \n",
    "                                    (grouped_data['admin1'] == admin1) & \n",
    "                                    (grouped_data['month'] == month)][var].values\n",
    "            if len(mean_value) > 0:\n",
    "                future_data.at[index, var] = mean_value[0]\n",
    "\n",
    "    # Certaines valeurs sont manquantes car les combinaisons (country, admin1, month) n'existent pas\n",
    "    # on va donc créer un dataframe contenant les combinaisons uniques de (country, admin1, month) dans future_data\n",
    "    future_combinations = future_data[['country', 'admin1', 'month']].drop_duplicates()\n",
    "\n",
    "    # Puis un dataframe contenant les combinaisons uniques de (country, admin1, month) dans past_data\n",
    "    monthly_combinations = past_data[['country', 'admin1', 'month']].drop_duplicates()\n",
    "\n",
    "    # Ensuite on recherche les combinaisons présentes dans future_data mais absentes dans past_data\n",
    "    missing_combinations = future_combinations[~future_combinations.isin(monthly_combinations)].dropna()\n",
    "\n",
    "    # Affichage des combinaisons manquantes\n",
    "    # print(\"Combinaisons manquantes dans past_data :\\n\"missing_combinations)\n",
    "\n",
    "    # En fonction de ces combinaisons, on vient remplir les valeurs manquantes dans future_data\n",
    "    # à partir du calcul de la moyenne des autres mois pour la combinaison (country, admin1)\n",
    "    for index, row in missing_combinations.iterrows():\n",
    "        # Sélection des lignes correspondant à la combinaison de country et admin1 dans past_data\n",
    "        matching_rows = past_data[(past_data['country'] == row['country']) & (past_data['admin1'] == row['admin1'])]\n",
    "        # Calcul de la moyenne des valeurs des variables pour tous les mois existants\n",
    "        mean_values = matching_rows[columns_to_fill].mean()\n",
    "        # Remplissage des valeurs manquantes dans future_data avec la moyenne calculée\n",
    "        future_data.loc[(future_data['country'] == row['country']) & (future_data['admin1'] == row['admin1']) & (future_data['month'] == row['month']), columns_to_fill] = mean_values.values\n",
    "\n",
    "    # Enfin on remplace les NaN des lignes restantes par la valeur la plus fréquente dans chaque colonne\n",
    "    future_data = future_data.fillna(future_data.mode().iloc[0])\n",
    "\n",
    "    # On supprime la colonne de variable cible et on vérifie qu'il n'y a plus de NaN\n",
    "    future_data = future_data.drop(columns=['terrorist_events'])\n",
    "    future_data.info()\n",
    "\n",
    "    # On arrondit les valeurs calculées pour les variables explicatives afin d'obtenir des valeurs entières\n",
    "    future_data = future_data.round(0)\n",
    "\n",
    "    return future_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSFORMATION DES DONNEES DU DATAFRAME \"future_data\"\n",
    "############################################################\n",
    "\n",
    "def future_data_transformation(df, scaler):\n",
    "    ##\n",
    "    # Préparation du dataframe \"future_data\" pour le modèle de ML\n",
    "    future_data_encoded = df.copy()\n",
    "\n",
    "    # On standardise les variables numériques exceptées les variables catégorielles\n",
    "    # ainsi que l'année et le mois qui subiront une autre transformation\n",
    "    cat_columns=['country', 'admin1']\n",
    "    future_data_numerical_vars = [col for col in future_data_encoded.columns if col not in cat_columns + ['year', 'month']]\n",
    "    # Extraction des données numériques\n",
    "    future_data_numerical = df[future_data_numerical_vars].values\n",
    "\n",
    "    # On utilise le scaler ajusté sur les données d'entraînement pour transformer les données du dataframe\n",
    "    future_data_encoded[future_data_numerical_vars] = scaler.transform(future_data_numerical)\n",
    "\n",
    "    # Hot One Encoding des variables catégorielles\n",
    "    future_data_encoded = pd.get_dummies(future_data_encoded, columns=cat_columns, dtype=int)\n",
    "\n",
    "    # Transformation trigonométrique de 'year' et 'month' pour la saisonnalité\n",
    "    future_data_encoded['month_sin'] = np.sin(2 * np.pi * future_data_encoded['month'] / 12)\n",
    "    future_data_encoded['month_cos'] = np.cos(2 * np.pi * future_data_encoded['month'] / 12)\n",
    "    future_data_encoded['year_sin'] = np.sin(2 * np.pi * (future_data_encoded['year'] - future_data_encoded['year'].min()) / \n",
    "                                    (future_data_encoded['year'].max() - future_data_encoded['year'].min() + 1))\n",
    "    future_data_encoded['year_cos'] = np.cos(2 * np.pi * (future_data_encoded['year'] - future_data_encoded['year'].min()) / \n",
    "                                    (future_data_encoded['year'].max() - future_data_encoded['year'].min() + 1))\n",
    "\n",
    "    # Suppression des colonnes 'month' et 'year' après transformation\n",
    "    future_data_encoded = future_data_encoded.drop(columns=['month', 'year'])\n",
    "\n",
    "    return future_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186 entries, 0 to 185\n",
      "Data columns (total 26 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   country                     186 non-null    object \n",
      " 1   admin1                      186 non-null    object \n",
      " 2   year                        186 non-null    int64  \n",
      " 3   month                       186 non-null    int64  \n",
      " 4   fatalities                  186 non-null    float64\n",
      " 5   Civilians                   186 non-null    float64\n",
      " 6   External/Other Forces       186 non-null    float64\n",
      " 7   Identity Militias           186 non-null    float64\n",
      " 8   Political Militias          186 non-null    float64\n",
      " 9   State Forces                186 non-null    float64\n",
      " 10  Battles                     186 non-null    float64\n",
      " 11  Explosions/Remote violence  186 non-null    float64\n",
      " 12  Strategic developments      186 non-null    float64\n",
      " 13  Violence against civilians  186 non-null    float64\n",
      " 14  total_events_lag_1          186 non-null    float64\n",
      " 15  terrorist_events_lag_1      186 non-null    float64\n",
      " 16  total_events_lag_2          186 non-null    float64\n",
      " 17  terrorist_events_lag_2      186 non-null    float64\n",
      " 18  total_events_lag_3          186 non-null    float64\n",
      " 19  terrorist_events_lag_3      186 non-null    float64\n",
      " 20  total_events_lag_4          186 non-null    float64\n",
      " 21  terrorist_events_lag_4      186 non-null    float64\n",
      " 22  total_events_lag_5          186 non-null    float64\n",
      " 23  terrorist_events_lag_5      186 non-null    float64\n",
      " 24  total_events_lag_6          186 non-null    float64\n",
      " 25  terrorist_events_lag_6      186 non-null    float64\n",
      "dtypes: float64(22), int64(2), object(2)\n",
      "memory usage: 37.9+ KB\n"
     ]
    }
   ],
   "source": [
    "## CREATION DES DATAFRAMES HISTORIQUES ET FUTURS\n",
    "############################################################\n",
    "\n",
    "# On applique les différentes étapes de préparation à notre dataframe initial\n",
    "df = load_dataset('../data/raw/terrorisme_sahel.csv')\n",
    "df = columns_filtering(df)\n",
    "df = add_new_colums_dates(df)\n",
    "df = add_new_colums_actors(df)\n",
    "df = add_new_colums_interaction(df)\n",
    "df = add_new_colums_terrorist_group(df)\n",
    "df = add_new_colums_pmc_group(df)\n",
    "df = delete_columns(df)\n",
    "df = fill_nan_values(df)\n",
    "\n",
    "# On crée notre dataframe qui sera utilisé pour entrainer notre modèle\n",
    "# sur les données historiques\n",
    "past_data = build_past_data(df)\n",
    "\n",
    "# On crée notre dataframe qui sera utilisé pour effectuer les prédictions\n",
    "# sur les 6 prochains mois\n",
    "future_data = build_future_data(past_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ETUDE DE LINEARITE\n",
    "############################################################\n",
    "    \n",
    "\n",
    "def encode_and_transform(df):\n",
    "    df_encoded = df.copy()\n",
    "    # On standardise les variables numériques exceptées les variables catégorielles\n",
    "    # ainsi que l'année et le mois qui subiront une autre transformation\n",
    "    cat_columns=['country', 'admin1']\n",
    "    df_encoded_numerical_vars = [col for col in df_encoded.columns if col not in cat_columns + ['year', 'month']]\n",
    "    # Extraction des données numériques\n",
    "    df_encoded_numerical = df_encoded[df_encoded_numerical_vars].values\n",
    "\n",
    "    # Initialisation d'un scaler\n",
    "    scaler = StandardScaler()\n",
    "    df_encoded[df_encoded_numerical_vars] = scaler.fit_transform(df_encoded_numerical)\n",
    "\n",
    "    # Hot One Encoding des variables catégorielles\n",
    "    df_encoded = pd.get_dummies(df_encoded, columns=cat_columns, dtype=int)\n",
    "\n",
    "    # Transformation trigonométrique de 'year' et 'month' pour la saisonnalité\n",
    "    df_encoded['month_sin'] = np.sin(2 * np.pi * df_encoded['month'] / 12)\n",
    "    df_encoded['month_cos'] = np.cos(2 * np.pi * df_encoded['month'] / 12)\n",
    "    df_encoded['year_sin'] = np.sin(2 * np.pi * (df_encoded['year'] - df_encoded['year'].min()) / \n",
    "                                    (df_encoded['year'].max() - df_encoded['year'].min() + 1))\n",
    "    df_encoded['year_cos'] = np.cos(2 * np.pi * (df_encoded['year'] - df_encoded['year'].min()) / \n",
    "                                    (df_encoded['year'].max() - df_encoded['year'].min() + 1))\n",
    "\n",
    "    # Suppression des colonnes 'month' et 'year' après transformation\n",
    "    df_encoded = df_encoded.drop(columns=['month', 'year'])\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "# Calculer la matrice de corrélation et afficher la heatmap\n",
    "corr_matrix = encode_and_transform(past_data).corr()\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Heatmap des Corrélations')\n",
    "plt.show();\n",
    "\n",
    "# Suite à cette étude on a décidé de supprimer la variable 'Rebel Groups' car corrélation à 1 avec 'terrorist events'\n",
    "# On a également supprimé la variable 'total_events' car corrlélée à 98% à 'Rebel Groups', 'terrorist events' et 'Civilians'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre visualisation des variables 2 à deux par nuages de points\n",
    "plt.figure(figsize=(40, 40))\n",
    "liste_colonnes = ['admin1','year','month', 'terrorist_events', 'fatalities', 'Civilians', 'External/Other Forces', 'Identity Militias','Political Militias', 'State Forces',\n",
    "                  'Battles', 'Explosions/Remote violence', 'Strategic developments', 'Violence against civilians']\n",
    "sns.pairplot(past_data[liste_colonnes])\n",
    "plt.title('Nuages d epoints des variables 2 à 2')\n",
    "plt.show()\n",
    "\n",
    "# on voit qu'aucune variable explicative ne suit de loi normale\n",
    "# une partie des variables semblent avoir des relations linéaires entre elles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE RANDOM FOREST REGRESSOR\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "# On instancie un modèle Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# On entraîne notre modèle\n",
    "rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = rf.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_rf = rf.score(X_test_encoded, y_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "pearson_corr_rf, _ = pearsonr(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest R2:\", r2_rf)\n",
    "print(\"Random Forest RMSE:\", rmse_rf)\n",
    "print(\"Random Forest MAE:\", mae_rf)\n",
    "print(\"Random Forest Pearson Correlation:\", pearson_corr_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE RANDOM FOREST REGRESSOR\n",
    "## AVEC GRIDSEARCH\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "# On définit la grille d'hyperparamètres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20, 50],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# On instancie le modèle Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# On instancie le GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# On entraîne le GridSearchCV\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Meilleurs paramètres trouvés par GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Meilleurs paramètres du GridSearchCV:\", best_params)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = grid_search.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_rf = grid_search.score(X_test_encoded, y_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "pearson_corr_rf, _ = pearsonr(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Gridsearch R2:\", r2_rf)\n",
    "print(\"Random Forest Gridsearch RMSE:\", rmse_rf)\n",
    "print(\"Random Forest Gridsearch MAE:\", mae_rf)\n",
    "print(\"Random Forest Gridsearch Pearson Correlation:\", pearson_corr_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE RANDOM FOREST REGRESSOR\n",
    "## AVEC REDUCTION DE DIMENSION (PCA, LDA, UMAP)\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "##\n",
    "# PCA\n",
    "pca = PCA()\n",
    "#pca = PCA(n_components = 0.9)\n",
    "X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "X_test_pca = pca.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle RF avec la PCA\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Prédictions de la PCA\n",
    "y_pred_pca = rf.predict(X_test_pca)\n",
    "\n",
    "# Évaluation de la PCA sur les données historiques\n",
    "r2_pca = rf.score(X_test_pca, y_test)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "pearson_corr_pca, _ = pearsonr(y_test, y_pred_pca)\n",
    "\n",
    "print(\"Random Forest PCA R2:\", r2_pca)\n",
    "print(\"Random Forest PCA RMSE:\", rmse_pca)\n",
    "print(\"Random Forest PCA MAE:\", mae_pca)\n",
    "print(\"Random Forest PCA Pearson Correlation:\", pearson_corr_pca)\n",
    "\n",
    "\n",
    "##\n",
    "# LDA\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train_encoded, y_train)\n",
    "X_test_lda = lda.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle RF avec la LDA\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf.fit(X_train_lda, y_train)\n",
    "\n",
    "# Prédictions de la LDA\n",
    "y_pred_lda = rf.predict(X_test_lda)\n",
    "\n",
    "# Évaluation de la LDA sur les données historiques\n",
    "r2_lda = rf.score(X_test_lda, y_test)\n",
    "rmse_lda = np.sqrt(mean_squared_error(y_test, y_pred_lda))\n",
    "mae_lda = mean_absolute_error(y_test, y_pred_lda)\n",
    "pearson_corr_lda, _ = pearsonr(y_test, y_pred_lda)\n",
    "\n",
    "print(\"\\nRandom Forest LDA R2:\", r2_lda)\n",
    "print(\"Random Forest LDA RMSE:\", rmse_lda)\n",
    "print(\"Random Forest LDA MAE:\", mae_lda)\n",
    "print(\"Random Forest LDA Pearson Correlation:\", pearson_corr_lda)\n",
    "\n",
    "\n",
    "##\n",
    "# UMAP\n",
    "\n",
    "# On cherche à l'aide d'un GridSearch les meilleurs paramètres\n",
    "params = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'min_dist': [0.1, 0.5, 0.9],\n",
    "    'n_components': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# On effectue une validation croisée pour évaluer les performances pour chaque combinaison de paramètres\n",
    "umap_cv = UMAP()\n",
    "grid_search = GridSearchCV(estimator=umap_cv, param_grid=params, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# On entraine le modèle avec le meilleur hyperparamètre\n",
    "best_umap_model = grid_search.best_estimator_\n",
    "X_train_umap = best_umap_model.fit_transform(X_train_encoded)\n",
    "X_test_umap = best_umap_model.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle RF sur les données réduites avec UMAP\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf.fit(X_train_umap, y_train)\n",
    "\n",
    "# Prédictions du modèle sur les données réduites avec UMAP\n",
    "y_pred_umap = rf.predict(X_test_umap)\n",
    "\n",
    "# Évaluation de UMAP sur les données historiques\n",
    "r2_umap = rf.score(X_test_umap, y_test)\n",
    "rmse_umap = np.sqrt(mean_squared_error(y_test, y_pred_umap))\n",
    "mae_umap = mean_absolute_error(y_test, y_pred_umap)\n",
    "pearson_corr_umap, _ = pearsonr(y_test, y_pred_umap)\n",
    "\n",
    "print(\"\\nRandom Forest UMAP R2:\", r2_umap)\n",
    "print(\"Random Forest UMAP RMSE:\", rmse_umap)\n",
    "print(\"Random Forest UMAP MAE:\", mae_umap)\n",
    "print(\"Random Forest UMAP Pearson Correlation:\", pearson_corr_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE RANDOM FOREST REGRESSOR\n",
    "## AVEC REDUCTION DE DIMENSION (PCA, LDA, UMAP)\n",
    "## AVEC GRIDSEARCH\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# PCA\n",
    "pca = PCA()\n",
    "#pca = PCA(n_components = 0.9)\n",
    "X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "X_test_pca = pca.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle RF avec la PCA\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(max_depth=30, max_features='sqrt', min_samples_leaf=1,\n",
    "                           min_samples_split=2, n_estimators=500, random_state=42)\n",
    "\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Prédictions de la PCA\n",
    "y_pred_pca = rf.predict(X_test_pca)\n",
    "\n",
    "# Évaluation de la PCA sur les données historiques\n",
    "r2_pca = rf.score(X_test_pca, y_test)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "pearson_corr_pca, _ = pearsonr(y_test, y_pred_pca)\n",
    "\n",
    "print(\"Random Forest PCA Gridsearch R2:\", r2_pca)\n",
    "print(\"Random Forest PCA Gridsearch RMSE:\", rmse_pca)\n",
    "print(\"Random Forest PCA Gridsearch MAE:\", mae_pca)\n",
    "print(\"Random Forest PCA Gridsearch Pearson Correlation:\", pearson_corr_pca)\n",
    "\n",
    "\n",
    "##\n",
    "# LDA\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train_encoded, y_train)\n",
    "X_test_lda = lda.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle RF avec la LDA\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(max_depth=30, max_features='sqrt', min_samples_leaf=1,\n",
    "                           min_samples_split=2, n_estimators=500, random_state=42)\n",
    "\n",
    "rf.fit(X_train_lda, y_train)\n",
    "\n",
    "# Prédictions de la LDA\n",
    "y_pred_lda = rf.predict(X_test_lda)\n",
    "\n",
    "# Évaluation de la LDA sur les données historiques\n",
    "r2_lda = rf.score(X_test_lda, y_test)\n",
    "rmse_lda = np.sqrt(mean_squared_error(y_test, y_pred_lda))\n",
    "mae_lda = mean_absolute_error(y_test, y_pred_lda)\n",
    "pearson_corr_lda, _ = pearsonr(y_test, y_pred_lda)\n",
    "\n",
    "print(\"\\nRandom Forest LDA Gridsearch R2:\", r2_lda)\n",
    "print(\"Random Forest LDA Gridsearch RMSE:\", rmse_lda)\n",
    "print(\"Random Forest LDA Gridsearch MAE:\", mae_lda)\n",
    "print(\"Random Forest LDA Gridsearch Pearson Correlation:\", pearson_corr_lda)\n",
    "\n",
    "\n",
    "##\n",
    "# UMAP\n",
    "\n",
    "# On cherche à l'aide d'un GridSearch les meilleurs paramètres\n",
    "params = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'min_dist': [0.1, 0.5, 0.9],\n",
    "    'n_components': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# On effectue une validation croisée pour évaluer les performances pour chaque combinaison de paramètres\n",
    "umap_cv = UMAP()\n",
    "grid_search = GridSearchCV(estimator=umap_cv, param_grid=params, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# On entraine le modèle avec le meilleur hyperparamètre\n",
    "best_umap_model = grid_search.best_estimator_\n",
    "X_train_umap = best_umap_model.fit_transform(X_train_encoded)\n",
    "X_test_umap = best_umap_model.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle RF sur les données réduites avec UMAP\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(max_depth=30, max_features='sqrt', min_samples_leaf=1,\n",
    "                           min_samples_split=2, n_estimators=500, random_state=42)\n",
    "\n",
    "rf.fit(X_train_umap, y_train)\n",
    "\n",
    "# Prédictions du modèle sur les données réduites avec UMAP\n",
    "y_pred_umap = rf.predict(X_test_umap)\n",
    "\n",
    "# Évaluation de UMAP sur les données historiques\n",
    "r2_umap = rf.score(X_test_umap, y_test)\n",
    "rmse_umap = np.sqrt(mean_squared_error(y_test, y_pred_umap))\n",
    "mae_umap = mean_absolute_error(y_test, y_pred_umap)\n",
    "pearson_corr_umap, _ = pearsonr(y_test, y_pred_umap)\n",
    "\n",
    "print(\"\\nRandom Forest UMAP Gridsearch R2:\", r2_umap)\n",
    "print(\"Random Forest UMAP Gridsearch RMSE:\", rmse_umap)\n",
    "print(\"Random Forest UMAP Gridsearch MAE:\", mae_umap)\n",
    "print(\"Random Forest UMAP Gridsearch Pearson Correlation:\", pearson_corr_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE RANDOM FOREST REGRESSOR\n",
    "## AVEC BOOSTING ET BAGGING\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# Boosting\n",
    "\n",
    "# On instancie un modèle Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# On entraîne notre modèle\n",
    "rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = rf.predict(X_test_encoded)\n",
    "\n",
    "ar = AdaBoostRegressor(estimator=rf, n_estimators=400)\n",
    "ar.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_ar = ar.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_ar = ar.score(X_test_encoded, y_test)\n",
    "rmse_ar = np.sqrt(mean_squared_error(y_test, y_pred_ar))\n",
    "mae_ar = mean_absolute_error(y_test, y_pred_ar)\n",
    "pearson_corr_ar, _ = pearsonr(y_test, y_pred_ar)\n",
    "\n",
    "print(\"Random Forest Boosting R2:\", r2_ar)\n",
    "print(\"Random Forest Boosting RMSE:\", rmse_ar)\n",
    "print(\"Random Forest Boosting MAE:\", mae_ar)\n",
    "print(\"Random Forest Boosting Pearson Correlation:\", pearson_corr_ar)\n",
    "\n",
    "\n",
    "##\n",
    "# Bagging\n",
    "\n",
    "# On instancie un modèle Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# On entraîne notre modèle\n",
    "rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = rf.predict(X_test_encoded)\n",
    "\n",
    "br = BaggingRegressor(n_estimators=1000, oob_score=True)\n",
    "br.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_br = br.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_br = br.score(X_test_encoded, y_test)\n",
    "rmse_br = np.sqrt(mean_squared_error(y_test, y_pred_br))\n",
    "mae_br = mean_absolute_error(y_test, y_pred_br)\n",
    "pearson_corr_br, _ = pearsonr(y_test, y_pred_br)\n",
    "\n",
    "print(\"\\nRandom Forest Bagging R2:\", r2_br)\n",
    "print(\"Random Forest Bagging RMSE:\", rmse_br)\n",
    "print(\"Random Forest Bagging MAE:\", mae_br)\n",
    "print(\"Random Forest Bagging Pearson Correlation:\", pearson_corr_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE RANDOM FOREST REGRESSOR\n",
    "## AVEC BOOSTING ET BAGGING\n",
    "## AVEC GRIDSEARCH\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# Boosting\n",
    "\n",
    "# On instancie un modèle Random Forest\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(max_depth=30, max_features='sqrt', min_samples_leaf=1,\n",
    "                           min_samples_split=2, n_estimators=500, random_state=42)\n",
    "\n",
    "# On entraîne notre modèle\n",
    "rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = rf.predict(X_test_encoded)\n",
    "\n",
    "ar = AdaBoostRegressor(estimator=rf, n_estimators=400)\n",
    "ar.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_ar = ar.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_ar = ar.score(X_test_encoded, y_test)\n",
    "rmse_ar = np.sqrt(mean_squared_error(y_test, y_pred_ar))\n",
    "mae_ar = mean_absolute_error(y_test, y_pred_ar)\n",
    "pearson_corr_ar, _ = pearsonr(y_test, y_pred_ar)\n",
    "\n",
    "print(\"Random Forest Boosting Gridsearch R2:\", r2_ar)\n",
    "print(\"Random Forest Boosting Gridsearch RMSE:\", rmse_ar)\n",
    "print(\"Random Forest Boosting Gridsearch MAE:\", mae_ar)\n",
    "print(\"Random Forest Boosting Gridsearch Pearson Correlation:\", pearson_corr_ar)\n",
    "\n",
    "\n",
    "##\n",
    "# Bagging\n",
    "\n",
    "# On instancie un modèle Random Forest\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "rf = RandomForestRegressor(max_depth=30, max_features='sqrt', min_samples_leaf=1,\n",
    "                           min_samples_split=2, n_estimators=500, random_state=42)\n",
    "\n",
    "# On entraîne notre modèle\n",
    "rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = rf.predict(X_test_encoded)\n",
    "\n",
    "br = BaggingRegressor(n_estimators=1000, oob_score=True)\n",
    "br.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_br = br.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_br = br.score(X_test_encoded, y_test)\n",
    "rmse_br = np.sqrt(mean_squared_error(y_test, y_pred_br))\n",
    "mae_br = mean_absolute_error(y_test, y_pred_br)\n",
    "pearson_corr_br, _ = pearsonr(y_test, y_pred_br)\n",
    "\n",
    "print(\"\\nRandom Forest Bagging Gridsearch R2:\", r2_br)\n",
    "print(\"Random Forest Bagging Gridsearch RMSE:\", rmse_br)\n",
    "print(\"Random Forest Bagging Gridsearch MAE:\", mae_br)\n",
    "print(\"Random Forest Bagging Gridsearch Pearson Correlation:\", pearson_corr_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE XGBOOST REGRESSOR\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "# On instancie un modèle XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "\n",
    "# On entraîne notre modèle\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_xgb = xgb_model.score(X_test_encoded, y_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "pearson_corr_xgb, _ = pearsonr(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost R2:\", r2_xgb)\n",
    "print(\"XGBoost RMSE:\", rmse_xgb)\n",
    "print(\"XGBoost MAE:\", mae_xgb)\n",
    "print(\"XGBoost Pearson Correlation:\", pearson_corr_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE XGBOOST REGRESSOR\n",
    "## AVEC GRIDSEARCH\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# On instancie le modèle XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# On instancie le GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# On entraîne le GridSearchCV\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Meilleurs paramètres trouvés par GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_params)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_xgb = grid_search.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_xgb = grid_search.score(X_test_encoded, y_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "pearson_corr_xgb, _ = pearsonr(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Gridsearch R2:\", r2_xgb)\n",
    "print(\"XGBoost Gridsearch RMSE:\", rmse_xgb)\n",
    "print(\"XGBoost Gridsearch MAE:\", mae_xgb)\n",
    "print(\"XGBoost Gridsearch Pearson Correlation:\", pearson_corr_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE XGBOOST REGRESSOR\n",
    "## AVEC REDUCTION DE DIMENSION (PCA, LDA, UMAP)\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# PCA\n",
    "\n",
    "pca = PCA()\n",
    "#pca = PCA(n_components = 0.9)\n",
    "X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "X_test_pca = pca.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle Xgboost avec la PCA\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Prédictions de la PCA\n",
    "y_pred_pca = xgb_model.predict(X_test_pca)\n",
    "\n",
    "# Évaluation de la PCA sur les données historiques\n",
    "r2_pca = xgb_model.score(X_test_pca, y_test)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "pearson_corr_pca, _ = pearsonr(y_test, y_pred_pca)\n",
    "\n",
    "print(\"XGBoost PCA R2:\", r2_pca)\n",
    "print(\"XGBoost PCA RMSE:\", rmse_pca)\n",
    "print(\"XGBoost PCA MAE:\", mae_pca)\n",
    "print(\"XGBoost PCA Pearson Correlation:\", pearson_corr_pca)\n",
    "\n",
    "\n",
    "##\n",
    "# LDA\n",
    "\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train_encoded, y_train)\n",
    "X_test_lda = lda.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle Xgboost avec la LDA\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_lda, y_train)\n",
    "\n",
    "# Prédictions de la LDA\n",
    "y_pred_lda = xgb_model.predict(X_test_lda)\n",
    "\n",
    "# Évaluation de la LDA sur les données historiques\n",
    "r2_lda = xgb_model.score(X_test_lda, y_test)\n",
    "rmse_lda = np.sqrt(mean_squared_error(y_test, y_pred_lda))\n",
    "mae_lda = mean_absolute_error(y_test, y_pred_lda)\n",
    "pearson_corr_lda, _ = pearsonr(y_test, y_pred_lda)\n",
    "\n",
    "print(\"\\nXGBoost LDA R2:\", r2_lda)\n",
    "print(\"XGBoost LDA RMSE:\", rmse_lda)\n",
    "print(\"XGBoost LDA MAE:\", mae_lda)\n",
    "print(\"XGBoost LDA Pearson Correlation:\", pearson_corr_lda)\n",
    "\n",
    "\n",
    "##\n",
    "# UMAP\n",
    "\n",
    "# On cherche à l'aide d'un GridSearch les meilleurs paramètres\n",
    "params = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'min_dist': [0.1, 0.5, 0.9],\n",
    "    'n_components': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# On effectue une validation croisée pour évaluer les performances pour chaque combinaison de paramètres\n",
    "umap_cv = UMAP()\n",
    "grid_search = GridSearchCV(estimator=umap_cv, param_grid=params, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# On entraine le modèle avec le meilleur hyperparamètre\n",
    "best_umap_model = grid_search.best_estimator_\n",
    "X_train_umap = best_umap_model.fit_transform(X_train_encoded)\n",
    "X_test_umap = best_umap_model.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle Xgboost avec UMAP\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_umap, y_train)\n",
    "\n",
    "# Prédictions du modèle sur les données réduites avec UMAP\n",
    "y_pred_umap = xgb_model.predict(X_test_umap)\n",
    "\n",
    "# Évaluation de UMAP sur les données historiques\n",
    "r2_umap = xgb_model.score(X_test_umap, y_test)\n",
    "rmse_umap = np.sqrt(mean_squared_error(y_test, y_pred_umap))\n",
    "mae_umap = mean_absolute_error(y_test, y_pred_umap)\n",
    "pearson_corr_umap, _ = pearsonr(y_test, y_pred_umap)\n",
    "\n",
    "print(\"\\nXGBoost UMAP R2:\", r2_umap)\n",
    "print(\"XGBoost UMAP RMSE:\", rmse_umap)\n",
    "print(\"XGBoost UMAP MAE:\", mae_umap)\n",
    "print(\"XGBoost UMAP Pearson Correlation:\", pearson_corr_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE XGBOOST REGRESSOR\n",
    "## AVEC REDUCTION DE DIMENSION (PCA, LDA, UMAP)\n",
    "## AVEC GRIDSEARCH\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# PCA\n",
    "\n",
    "pca = PCA()\n",
    "#pca = PCA(n_components = 0.9)\n",
    "X_train_pca = pca.fit_transform(X_train_encoded)\n",
    "X_test_pca = pca.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle Xgboost avec la PCA\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth= 3,\n",
    "                             n_estimators= 300, reg_alpha= 0, reg_lambda= 2,\n",
    "                             subsample= 0.6, objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Prédictions de la PCA\n",
    "y_pred_pca = xgb_model.predict(X_test_pca)\n",
    "\n",
    "# Évaluation de la PCA sur les données historiques\n",
    "r2_pca = xgb_model.score(X_test_pca, y_test)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "pearson_corr_pca, _ = pearsonr(y_test, y_pred_pca)\n",
    "\n",
    "print(\"XGBoost PCA Gridsearch R2:\", r2_pca)\n",
    "print(\"XGBoost PCA Gridsearch RMSE:\", rmse_pca)\n",
    "print(\"XGBoost PCA Gridsearch MAE:\", mae_pca)\n",
    "print(\"XGBoost PCA Gridsearch Pearson Correlation:\", pearson_corr_pca)\n",
    "\n",
    "\n",
    "##\n",
    "# LDA\n",
    "\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train_encoded, y_train)\n",
    "X_test_lda = lda.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle Xgboost avec la LDA\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth= 3,\n",
    "                             n_estimators= 300, reg_alpha= 0, reg_lambda= 2,\n",
    "                             subsample= 0.6, objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_lda, y_train)\n",
    "\n",
    "# Prédictions de la LDA\n",
    "y_pred_lda = xgb_model.predict(X_test_lda)\n",
    "\n",
    "# Évaluation de la LDA sur les données historiques\n",
    "r2_lda = xgb_model.score(X_test_lda, y_test)\n",
    "rmse_lda = np.sqrt(mean_squared_error(y_test, y_pred_lda))\n",
    "mae_lda = mean_absolute_error(y_test, y_pred_lda)\n",
    "pearson_corr_lda, _ = pearsonr(y_test, y_pred_lda)\n",
    "\n",
    "print(\"\\nXGBoost LDA Gridsearch R2:\", r2_lda)\n",
    "print(\"XGBoost LDA Gridsearch RMSE:\", rmse_lda)\n",
    "print(\"XGBoost LDA Gridsearch MAE:\", mae_lda)\n",
    "print(\"XGBoost LDA Gridsearch Pearson Correlation:\", pearson_corr_lda)\n",
    "\n",
    "\n",
    "##\n",
    "# UMAP\n",
    "\n",
    "# On cherche à l'aide d'un GridSearch les meilleurs paramètres\n",
    "params = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'min_dist': [0.1, 0.5, 0.9],\n",
    "    'n_components': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# On effectue une validation croisée pour évaluer les performances pour chaque combinaison de paramètres\n",
    "umap_cv = UMAP()\n",
    "grid_search = GridSearchCV(estimator=umap_cv, param_grid=params, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# On entraine le modèle avec le meilleur hyperparamètre\n",
    "best_umap_model = grid_search.best_estimator_\n",
    "X_train_umap = best_umap_model.fit_transform(X_train_encoded)\n",
    "X_test_umap = best_umap_model.transform(X_test_encoded)\n",
    "\n",
    "# On entraine un modèle Xgboost avec UMAP\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth= 3,\n",
    "                             n_estimators= 300, reg_alpha= 0, reg_lambda= 2,\n",
    "                             subsample= 0.6, objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_umap, y_train)\n",
    "\n",
    "# Prédictions du modèle sur les données réduites avec UMAP\n",
    "y_pred_umap = xgb_model.predict(X_test_umap)\n",
    "\n",
    "# Évaluation de UMAP sur les données historiques\n",
    "r2_umap = xgb_model.score(X_test_umap, y_test)\n",
    "rmse_umap = np.sqrt(mean_squared_error(y_test, y_pred_umap))\n",
    "mae_umap = mean_absolute_error(y_test, y_pred_umap)\n",
    "pearson_corr_umap, _ = pearsonr(y_test, y_pred_umap)\n",
    "\n",
    "print(\"\\nXGBoost UMAP Gridsearch R2:\", r2_umap)\n",
    "print(\"XGBoost UMAP Gridsearch RMSE:\", rmse_umap)\n",
    "print(\"XGBoost UMAP Gridsearch MAE:\", mae_umap)\n",
    "print(\"XGBoost UMAP Gridsearch Pearson Correlation:\", pearson_corr_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE XGBOOST\n",
    "## AVEC BOOSTING ET BAGGING\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# Boosting\n",
    "\n",
    "# On entraine un modèle Xgboost avec UMAP\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "ar = AdaBoostRegressor(estimator=xgb_model, n_estimators=400)\n",
    "\n",
    "ar.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_ar = ar.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_ar = ar.score(X_test_encoded, y_test)\n",
    "rmse_ar = np.sqrt(mean_squared_error(y_test, y_pred_ar))\n",
    "mae_ar = mean_absolute_error(y_test, y_pred_ar)\n",
    "pearson_corr_ar, _ = pearsonr(y_test, y_pred_ar)\n",
    "\n",
    "print(\"XGBoost Boosting R2:\", r2_ar)\n",
    "print(\"XGBoost Boosting RMSE:\", rmse_ar)\n",
    "print(\"XGBoost Boosting MAE:\", mae_ar)\n",
    "print(\"XGBoost Boosting Pearson Correlation:\", pearson_corr_ar)\n",
    "\n",
    "\n",
    "##\n",
    "# Bagging\n",
    "\n",
    "# On entraine un modèle Xgboost avec UMAP\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "br = BaggingRegressor(n_estimators=1000, oob_score=True)\n",
    "\n",
    "br.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_br = br.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_br = br.score(X_test_encoded, y_test)\n",
    "rmse_br = np.sqrt(mean_squared_error(y_test, y_pred_br))\n",
    "mae_br = mean_absolute_error(y_test, y_pred_br)\n",
    "pearson_corr_br, _ = pearsonr(y_test, y_pred_br)\n",
    "\n",
    "print(\"\\nXGBoost Bagging R2:\", r2_br)\n",
    "print(\"XGBoost Bagging RMSE:\", rmse_br)\n",
    "print(\"XGBoost Bagging MAE:\", mae_br)\n",
    "print(\"XGBoost Bagging Pearson Correlation:\", pearson_corr_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE XGBOOST\n",
    "## AVEC BOOSTING ET BAGGING\n",
    "## AVEC GRIDSEARCH\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "\n",
    "##\n",
    "# Boosting\n",
    "\n",
    "# On entraine un modèle Xgboost avec UMAP\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth= 3,\n",
    "                             n_estimators= 300, reg_alpha= 0, reg_lambda= 2,\n",
    "                             subsample= 0.6, objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "ar = AdaBoostRegressor(estimator=xgb_model, n_estimators=400)\n",
    "\n",
    "ar.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_ar = ar.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_ar = ar.score(X_test_encoded, y_test)\n",
    "rmse_ar = np.sqrt(mean_squared_error(y_test, y_pred_ar))\n",
    "mae_ar = mean_absolute_error(y_test, y_pred_ar)\n",
    "pearson_corr_ar, _ = pearsonr(y_test, y_pred_ar)\n",
    "\n",
    "print(\"XGBoost Boosting Gridsearch R2:\", r2_ar)\n",
    "print(\"XGBoost Boosting Gridsearch RMSE:\", rmse_ar)\n",
    "print(\"XGBoost Boosting Gridsearch MAE:\", mae_ar)\n",
    "print(\"XGBoost Boosting Gridsearch Pearson Correlation:\", pearson_corr_ar)\n",
    "\n",
    "\n",
    "##\n",
    "# Bagging\n",
    "\n",
    "# On entraine un modèle Xgboost avec UMAP\n",
    "# avec les meilleurs paramètres du Gridsearch\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth= 3,\n",
    "                             n_estimators= 300, reg_alpha= 0, reg_lambda= 2,\n",
    "                             subsample= 0.6, objective='reg:squarederror', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "br = BaggingRegressor(n_estimators=1000, oob_score=True)\n",
    "\n",
    "br.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_pred_br = br.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_br = br.score(X_test_encoded, y_test)\n",
    "rmse_br = np.sqrt(mean_squared_error(y_test, y_pred_br))\n",
    "mae_br = mean_absolute_error(y_test, y_pred_br)\n",
    "pearson_corr_br, _ = pearsonr(y_test, y_pred_br)\n",
    "\n",
    "print(\"\\nXGBoost Bagging Gridsearch R2:\", r2_br)\n",
    "print(\"XGBoost Bagging Gridsearch RMSE:\", rmse_br)\n",
    "print(\"XGBoost Bagging Gridsearch MAE:\", mae_br)\n",
    "print(\"XGBoost Bagging Gridsearch Pearson Correlation:\", pearson_corr_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODELE SVM REGRESSOR\n",
    "############################################################\n",
    "\n",
    "# On récupère nos jeux d'entrainement et de test\n",
    "X_train_encoded, X_test_encoded, _, y_train, y_test, _ = past_data_transformation(past_data)\n",
    "\n",
    "# On instancie un modèle SVR\n",
    "svr_model = svm.SVR()\n",
    "parametres = {'C':[1,10, 50, 100, 1000], 'kernel':['rbf', 'poly', 'sigmoid'], 'gamma':[0.001, 0.01, 0.1]}\n",
    "grid_svr_model = GridSearchCV(estimator=svr_model, param_grid=parametres, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# On entraine le modéle\n",
    "grille = grid_svr_model.fit(X_train_encoded,y_train)\n",
    "print(pd.DataFrame.from_dict(grille.cv_results_).loc[:,['params', 'mean_test_score']]) \n",
    "print(grid_svr_model.best_params_)\n",
    "\n",
    "#Prédiction sur l'ensemble de test\n",
    "y_pred_svr = grid_svr_model.predict(X_test_encoded)\n",
    "\n",
    "# Évaluation du modèle\n",
    "r2_svr_train = grid_svr_model.score(X_train_encoded, y_train)\n",
    "r2_svr_test = grid_svr_model.score(X_test_encoded, y_test)\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "mape_svr = mean_absolute_percentage_error(y_test, y_pred_svr)\n",
    "pearson_corr_svr, _ = pearsonr(y_test, y_pred_svr)\n",
    "\n",
    "print(\"SVR R2 train:\", r2_svr_train)\n",
    "print(\"SVR R2 test:\", r2_svr_test)\n",
    "print(\"SVR RMSE:\", rmse_svr)\n",
    "print(\"SVR MAE:\", mae_svr)\n",
    "print(\"SVR MAPE:\", mape_svr)\n",
    "print(\"Pearson Correlation (SVR):\", pearson_corr_svr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
